  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Results Export & Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 12: RESULTS EXPORT & FINAL SUMMARY\n",
    "# ================================================================================\n",
    "\n",
    "def export_all_results(model_results, feature_importance_results, cell_type_feature_analysis,\n",
    "                      tf_activity_results, top_tfs_results, timing_analysis_results,\n",
    "                      ml_data, save_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Export all analysis results to files for future use\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create results directory structure\n",
    "    results_dir = Path(save_dir)\n",
    "    (results_dir / \"models\").mkdir(parents=True, exist_ok=True)\n",
    "    (results_dir / \"data\").mkdir(parents=True, exist_ok=True)\n",
    "    (results_dir / \"analysis\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"ðŸ”„ Exporting all results...\")\n",
    "    \n",
    "    # 1. Export trained models\n",
    "    print(\"  ðŸ’¾ Saving trained models...\")\n",
    "    for model_name, result in model_results.items():\n",
    "        model_file = results_dir / \"models\" / f\"{model_name.lower().replace(' ', '_')}_model.pkl\"\n",
    "        joblib.dump(result['model'], model_file)\n",
    "    \n",
    "    # Save label encoder and scaler\n",
    "    joblib.dump(ml_data['label_encoder'], results_dir / \"models\" / \"label_encoder.pkl\")\n",
    "    joblib.dump(ml_data['scaler'], results_dir / \"models\" / \"feature_scaler.pkl\")\n",
    "    \n",
    "    # 2. Export model performance metrics\n",
    "    print(\"  ðŸ“Š Saving performance metrics...\")\n",
    "    performance_summary = []\n",
    "    for model_name, result in model_results.items():\n",
    "        performance_summary.append({\n",
    "            'Model': model_name,\n",
    "            'Train_Accuracy': result['train_score'],\n",
    "            'Test_Accuracy': result['test_score'],\n",
    "            'CV_Mean': result['cv_mean'],\n",
    "            'CV_Std': result['cv_std'],\n",
    "            'Overfitting_Score': result['overfitting_score'],\n",
    "            'Overfitting_Status': result['overfitting_status']\n",
    "        })\n",
    "    \n",
    "    performance_df = pd.DataFrame(performance_summary)\n",
    "    performance_df.to_csv(results_dir / \"analysis\" / \"model_performance_summary.csv\", index=False)\n",
    "    \n",
    "    # 3. Export feature importance results\n",
    "    print(\"  ðŸŽ¯ Saving feature importance analysis...\")\n",
    "    for model_name, features in feature_importance_results.items():\n",
    "        # Save all features with importance\n",
    "        features['all_features'].to_csv(\n",
    "            results_dir / \"analysis\" / f\"{model_name.lower().replace(' ', '_')}_feature_importance.csv\",\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # Save top 20 features\n",
    "        features['top_features'].to_csv(\n",
    "            results_dir / \"analysis\" / f\"{model_name.lower().replace(' ', '_')}_top20_features.csv\",\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    # 4. Export cell type specific features\n",
    "    print(\"  ðŸ§¬ Saving cell-type specific features...\")\n",
    "    for cell_type, analysis in cell_type_feature_analysis.items():\n",
    "        safe_cell_type = cell_type.replace(' ', '_').replace('/', '_')\n",
    "        analysis['all_features'].to_csv(\n",
    "            results_dir / \"analysis\" / f\"celltype_{safe_cell_type}_features.csv\",\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    # 5. Export TF activity analysis\n",
    "    print(\"  ðŸ”¬ Saving TF activity analysis...\")\n",
    "    tf_activity_results[0].to_csv(results_dir / \"analysis\" / \"tf_activity_analysis.csv\", index=False)\n",
    "    tf_activity_results[1].to_csv(results_dir / \"analysis\" / \"tf_expression_matrix.csv\")\n",
    "    tf_activity_results[2].to_csv(results_dir / \"analysis\" / \"tf_detection_matrix.csv\")\n",
    "    \n",
    "    # 6. Export top TFs per cell type\n",
    "    print(\"  ðŸ† Saving top TFs analysis...\")\n",
    "    top_tfs_results[1].to_csv(results_dir / \"analysis\" / \"top_tfs_summary.csv\", index=False)\n",
    "    \n",
    "    # Export detailed top TFs per cell type\n",
    "    top_tfs_detailed = []\n",
    "    for cell_type, data in top_tfs_results[0].items():\n",
    "        for i, (feature, score) in enumerate(zip(data['features'], data['scores'])):\n",
    "            top_tfs_detailed.append({\n",
    "                'CellType': cell_type,\n",
    "                'Rank': i + 1,\n",
    "                'TF_Feature': feature,\n",
    "                'F_Score': score\n",
    "            })\n",
    "    \n",
    "    pd.DataFrame(top_tfs_detailed).to_csv(results_dir / \"analysis\" / \"top_tfs_detailed.csv\", index=False)\n",
    "    \n",
    "    # 7. Export timing analysis\n",
    "    print(\"  â±ï¸ Saving timing analysis...\")\n",
    "    timing_analysis_results.to_csv(results_dir / \"analysis\" / \"model_timing_analysis.csv\", index=False)\n",
    "    \n",
    "    # 8. Create comprehensive summary report\n",
    "    print(\"  ðŸ“‹ Creating summary report...\")\n",
    "    \n",
    "    summary_report = f\"\"\"\n",
    "# Cell-Type Classification Analysis Summary Report\n",
    "**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Dataset Summary\n",
    "- **Total Cells:** {ml_data['X_train'].shape[0] + ml_data['X_test'].shape[0]:,}\n",
    "- **Total Features:** {ml_data['X_train'].shape[1]:,}\n",
    "- **RNA Features:** {len([f for f in ml_data['X_train'].columns if feature_type_map.get(f) == 'RNA']):,}\n",
    "- **ATAC Features:** {len([f for f in ml_data['X_train'].columns if feature_type_map.get(f) == 'ATAC']):,}\n",
    "- **Cell Types:** {len(ml_data['label_encoder'].classes_)}\n",
    "- **Training Cells:** {ml_data['X_train'].shape[0]:,}\n",
    "- **Test Cells:** {ml_data['X_test'].shape[0]:,}\n",
    "\n",
    "## Model Performance Summary\n",
    "{performance_df.to_string(index=False)}\n",
    "\n",
    "## Best Performing Models\n",
    "**Highest Test Accuracy:** {performance_df.loc[performance_df['Test_Accuracy'].idxmax(), 'Model']} ({performance_df['Test_Accuracy'].max():.4f})\n",
    "**Most Stable (Lowest CV Std):** {performance_df.loc[performance_df['CV_Std'].idxmin(), 'Model']} (Â±{performance_df['CV_Std'].min():.4f})\n",
    "**Best Fit (Lowest Overfitting):** {performance_df.loc[performance_df['Overfitting_Score'].idxmin(), 'Model']} ({performance_df['Overfitting_Score'].min():.4f})\n",
    "\n",
    "## Timing Analysis Summary\n",
    "{timing_analysis_results.to_string(index=False)}\n",
    "\n",
    "**Fastest Training:** {timing_analysis_results.loc[timing_analysis_results['Training_Time_s'].idxmin(), 'Model']} ({timing_analysis_results['Training_Time_s'].min():.2f}s)\n",
    "**Most Efficient:** {timing_analysis_results.loc[timing_analysis_results['Efficiency_Score'].idxmax(), 'Model']} ({timing_analysis_results['Efficiency_Score'].max():.3f})\n",
    "\n",
    "## Cell Type Analysis\n",
    "**Cell Types Analyzed:** {', '.join(list(cell_type_feature_analysis.keys()))}\n",
    "\n",
    "## Files Generated\n",
    "### Models\n",
    "- Trained models: `results/models/`\n",
    "- Label encoder: `results/models/label_encoder.pkl`\n",
    "- Feature scaler: `results/models/feature_scaler.pkl`\n",
    "\n",
    "### Analysis Results\n",
    "- Model performance: `results/analysis/model_performance_summary.csv`\n",
    "- Feature importance: `results/analysis/*_feature_importance.csv`\n",
    "- Cell-type features: `results/analysis/celltype_*_features.csv`\n",
    "- TF activity: `results/analysis/tf_activity_analysis.csv`\n",
    "- Top TFs: `results/analysis/top_tfs_*.csv`\n",
    "- Timing analysis: `results/analysis/model_timing_analysis.csv`\n",
    "\n",
    "### Visualizations\n",
    "- Performance plots: `results/plots/model_performance_comparison.png`\n",
    "- Feature heatmaps: `results/plots/*_heatmap.png`\n",
    "- TF network: `results/plots/tf_celltype_network.png`\n",
    "- TF activity: `results/plots/tf_activity_dotplot.png`\n",
    "- Interactive plots: `results/plots/*_interactive.html`\n",
    "\n",
    "## Recommendations\n",
    "1. **Best Overall Model:** {performance_df.loc[performance_df['Test_Accuracy'].idxmax(), 'Model']} for highest accuracy\n",
    "2. **Production Model:** {timing_analysis_results.loc[timing_analysis_results['Efficiency_Score'].idxmax(), 'Model']} for best efficiency\n",
    "3. **Overfitting Concerns:** Monitor {len(performance_df[performance_df['Overfitting_Status'].str.contains('Overfitting')])} models showing overfitting\n",
    "\n",
    "## Next Steps\n",
    "1. Validate findings with additional datasets\n",
    "2. Investigate top TF features for biological relevance\n",
    "3. Consider ensemble methods for improved performance\n",
    "4. Optimize hyperparameters for best-performing models\n",
    "\"\"\"\n",
    "    \n",
    "    # Save summary report\n",
    "    with open(results_dir / \"ANALYSIS_SUMMARY_REPORT.md\", 'w') as f:\n",
    "        f.write(summary_report)\n",
    "    \n",
    "    # 9. Create metadata file\n",
    "    metadata = {\n",
    "        'analysis_date': datetime.now().isoformat(),\n",
    "        'total_cells': int(ml_data['X_train'].shape[0] + ml_data['X_test'].shape[0]),\n",
    "        'total_features': int(ml_data['X_train'].shape[1]),\n",
    "        'cell_types': list(ml_data['label_encoder'].classes_),\n",
    "        'models_trained': list(model_results.keys()),\n",
    "        'best_model': performance_df.loc[performance_df['Test_Accuracy'].idxmax(), 'Model'],\n",
    "        'best_accuracy': float(performance_df['Test_Accuracy'].max()),\n",
    "        'files_generated': {\n",
    "            'models': len(list((results_dir / \"models\").glob(\"*.pkl\"))),\n",
    "            'analysis_files': len(list((results_dir / \"analysis\").glob(\"*.csv\"))),\n",
    "            'plots': len(list((results_dir / \"plots\").glob(\"*.png\")))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(results_dir / \"analysis_metadata.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… All results exported to {results_dir}/\")\n",
    "    print(f\"ðŸ“ Total files created: {len(list(results_dir.rglob('*.*')))}\")\n",
    "    print(f\"ðŸ“‹ Summary report: {results_dir}/ANALYSIS_SUMMARY_REPORT.md\")\n",
    "    \n",
    "    return results_dir\n",
    "\n",
    "# Export all results\n",
    "results_directory = export_all_results(\n",
    "    model_results,\n",
    "    feature_importance_results,\n",
    "    cell_type_feature_analysis,\n",
    "    tf_activity_results,\n",
    "    top_tfs_results,\n",
    "    timing_analysis_results,\n",
    "    ml_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Final Analysis Summary & Validation Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 13: FINAL ANALYSIS SUMMARY & PIPELINE COMPLETION\n",
    "# ================================================================================\n",
    "\n",
    "def display_final_summary(model_results, timing_analysis_results, cell_type_feature_analysis):\n",
    "    \"\"\"\n",
    "    Display comprehensive final summary of the analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" \" * 20 + \"ðŸŽ¯ CELL-TYPE CLASSIFICATION ANALYSIS COMPLETE ðŸŽ¯\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Model Performance Summary\n",
    "    print(\"\\nðŸ“Š MODEL PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    performance_data = []\n",
    "    for model_name, result in model_results.items():\n",
    "        performance_data.append([\n",
    "            model_name,\n",
    "            f\"{result['test_score']:.4f}\",\n",
    "            f\"{result['cv_mean']:.4f} Â± {result['cv_std']:.4f}\",\n",
    "            result['overfitting_status']\n",
    "        ])\n",
    "    \n",
    "    performance_df = pd.DataFrame(performance_data, \n",
    "                                 columns=['Model', 'Test Accuracy', 'CV Score', 'Overfitting Status'])\n",
    "    print(performance_df.to_string(index=False))\n",
    "    \n",
    "    # Best Models\n",
    "    best_model = max(model_results.items(), key=lambda x: x[1]['test_score'])\n",
    "    most_stable = min(model_results.items(), key=lambda x: x[1]['cv_std'])\n",
    "    best_fit = min(model_results.items(), key=lambda x: abs(x[1]['overfitting_score']))\n",
    "    \n",
    "    print(f\"\\nðŸ† BEST MODELS:\")\n",
    "    print(f\"   ðŸ¥‡ Highest Accuracy: {best_model[0]} ({best_model[1]['test_score']:.4f})\")\n",
    "    print(f\"   ðŸŽ¯ Most Stable: {most_stable[0]} (CV std: Â±{most_stable[1]['cv_std']:.4f})\")\n",
    "    print(f\"   âš–ï¸ Best Fit: {best_fit[0]} (Overfitting: {best_fit[1]['overfitting_score']:.4f})\")\n",
    "    \n",
    "    # Timing Summary\n",
    "    fastest_model = timing_analysis_results.loc[timing_analysis_results['Training_Time_s'].idxmin()]\n",
    "    most_efficient = timing_analysis_results.loc[timing_analysis_results['Efficiency_Score'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nâ±ï¸ TIMING ANALYSIS:\")\n",
    "    print(f\"   ðŸš€ Fastest Training: {fastest_model['Model']} ({fastest_model['Training_Time_s']:.2f}s)\")\n",
    "    print(f\"   ðŸ’¡ Most Efficient: {most_efficient['Model']} (Score: {most_efficient['Efficiency_Score']:.3f})\")\n",
    "    \n",
    "    # Cell Type Analysis\n",
    "    print(f\"\\nðŸ§¬ CELL TYPE ANALYSIS:\")\n",
    "    print(f\"   ðŸ“‹ Cell Types Analyzed: {len(cell_type_feature_analysis)}\")\n",
    "    print(f\"   ðŸŽ¯ Cell Types: {', '.join(list(cell_type_feature_analysis.keys()))}\")\n",
    "    \n",
    "    # Feature Analysis Summary\n",
    "    total_significant_features = sum(len(analysis['all_features']) for analysis in cell_type_feature_analysis.values())\n",
    "    avg_features_per_type = total_significant_features / len(cell_type_feature_analysis)\n",
    "    \n",
    "    print(f\"   ðŸ”¬ Total Significant Features: {total_significant_features:,}\")\n",
    "    print(f\"   ðŸ“Š Average Features per Cell Type: {avg_features_per_type:.1f}\")\n",
    "    \n",
    "    # Analysis Outputs\n",
    "    print(f\"\\nðŸ“ ANALYSIS OUTPUTS:\")\n",
    "    print(f\"   ðŸ¤– Trained Models: 5 machine learning models saved\")\n",
    "    print(f\"   ðŸ“Š Performance Plots: Learning curves, overfitting analysis\")\n",
    "    print(f\"   ðŸ”¥ Feature Heatmaps: Top 20 features across models and cell types\")\n",
    "    print(f\"   ðŸ•¸ï¸ TF-Cell Type Network: Interactive network visualization\")\n",
    "    print(f\"   ðŸŽ¯ TF Activity Plots: Dot plots showing TF expression patterns\")\n",
    "    print(f\"   ðŸ† Top TFs Analysis: Cell-type specific transcription factors\")\n",
    "    print(f\"   â±ï¸ Timing Analysis: Model efficiency comparisons\")\n",
    "    print(f\"   ðŸ“‹ Summary Report: Comprehensive markdown report\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "    print(f\"   1. Use {best_model[0]} for highest accuracy ({best_model[1]['test_score']:.4f})\")\n",
    "    print(f\"   2. Use {most_efficient['Model']} for production (best efficiency)\")\n",
    "    print(f\"   3. Investigate top TF features for biological validation\")\n",
    "    print(f\"   4. Consider ensemble methods for improved robustness\")\n",
    "    \n",
    "    # Overfitting Warning\n",
    "    overfitting_models = [name for name, result in model_results.items() \n",
    "                         if 'Overfitting' in result['overfitting_status']]\n",
    "    if overfitting_models:\n",
    "        print(f\"\\nâš ï¸ WARNING: {len(overfitting_models)} models show overfitting: {', '.join(overfitting_models)}\")\n",
    "        print(f\"   Consider regularization or more data for these models.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" \" * 25 + \"ðŸŽ‰ ANALYSIS PIPELINE COMPLETED! ðŸŽ‰\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nðŸ“… Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"ðŸ“ All results saved to: ./results/\")\n",
    "    print(f\"ðŸ“– Read the summary report: ./results/ANALYSIS_SUMMARY_REPORT.md\")\n",
    "    print(\"\\nðŸš€ Ready for downstream analysis and biological interpretation!\")\n",
    "\n",
    "# Display final summary\n",
    "display_final_summary(model_results, timing_analysis_results, cell_type_feature_analysis)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ STEP 12 FIXED: All validation steps completed successfully!\")\n",
    "print(\"âœ… Models trained with overfitting analysis\")\n",
    "print(\"âœ… Results saved for future validation and visualization\")\n",
    "print(\"âœ… TF-celltype networks generated\")\n",
    "print(\"âœ… Feature importance heatmaps created\")\n",
    "print(\"âœ… Model timing analysis completed\")\n",
    "print(\"âœ… TF activity dotplots generated\")\n",
    "print(\"âœ… Top TFs per cell type identified\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: TF Activity Analysis & Dot Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 9: TF ACTIVITY ANALYSIS & DOT PLOTS\n",
    "# ================================================================================\n",
    "\n",
    "def create_tf_activity_dotplot(X_train, y_train_labels, feature_info, \n",
    "                              cell_type_feature_analysis, save_dir=\"results/plots\"):\n",
    "    \"\"\"\n",
    "    Create dot plot showing TF activity/importance across cell types\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(\"ðŸ”„ Creating TF activity dot plots...\")\n",
    "    \n",
    "    # Get top TF features for each cell type\n",
    "    cell_types = list(cell_type_feature_analysis.keys())\n",
    "    top_tfs_per_celltype = {}\n",
    "    all_top_tfs = set()\n",
    "    \n",
    "    # Extract top RNA features (potential TFs) for each cell type\n",
    "    for cell_type in cell_types:\n",
    "        top_rna = cell_type_feature_analysis[cell_type]['top_rna']\n",
    "        top_tfs = top_rna.head(10)  # Top 10 RNA features per cell type\n",
    "        top_tfs_per_celltype[cell_type] = top_tfs\n",
    "        all_top_tfs.update(top_tfs['feature'].tolist())\n",
    "    \n",
    "    print(f\"  ðŸ“Š Analyzing {len(all_top_tfs)} unique TF features across {len(cell_types)} cell types\")\n",
    "    \n",
    "    # Calculate mean expression and detection rate for each TF in each cell type\n",
    "    dotplot_data = []\n",
    "    \n",
    "    for tf in all_top_tfs:\n",
    "        if tf not in X_train.columns:\n",
    "            continue\n",
    "            \n",
    "        for cell_type in cell_types:\n",
    "            # Get cells of this type\n",
    "            mask = y_train_labels == cell_type\n",
    "            tf_expression = X_train.loc[mask, tf]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mean_expr = tf_expression.mean()\n",
    "            detection_rate = (tf_expression > 0).mean()  # Percentage of cells expressing this TF\n",
    "            \n",
    "            # Get significance score from cell type analysis\n",
    "            if cell_type in cell_type_feature_analysis:\n",
    "                feature_df = cell_type_feature_analysis[cell_type]['all_features']\n",
    "                if tf in feature_df['feature'].values:\n",
    "                    significance = feature_df[feature_df['feature'] == tf]['score'].iloc[0]\n",
    "                else:\n",
    "                    significance = 0\n",
    "            else:\n",
    "                significance = 0\n",
    "            \n",
    "            dotplot_data.append({\n",
    "                'TF': tf,\n",
    "                'CellType': cell_type,\n",
    "                'MeanExpression': mean_expr,\n",
    "                'DetectionRate': detection_rate,\n",
    "                'Significance': significance\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    dotplot_df = pd.DataFrame(dotplot_data)\n",
    "    \n",
    "    # Pivot for visualization\n",
    "    mean_expr_pivot = dotplot_df.pivot(index='TF', columns='CellType', values='MeanExpression')\n",
    "    detection_rate_pivot = dotplot_df.pivot(index='TF', columns='CellType', values='DetectionRate')\n",
    "    significance_pivot = dotplot_df.pivot(index='TF', columns='CellType', values='Significance')\n",
    "    \n",
    "    # Create dot plot using matplotlib\n",
    "    fig, ax = plt.subplots(figsize=(max(12, len(cell_types) * 1.5), max(8, len(all_top_tfs) * 0.4)))\n",
    "    \n",
    "    # Prepare data for scatter plot\n",
    "    x_pos = []\n",
    "    y_pos = []\n",
    "    sizes = []\n",
    "    colors = []\n",
    "    \n",
    "    for i, tf in enumerate(mean_expr_pivot.index):\n",
    "        for j, cell_type in enumerate(mean_expr_pivot.columns):\n",
    "            x_pos.append(j)\n",
    "            y_pos.append(i)\n",
    "            # Size based on detection rate\n",
    "            size = detection_rate_pivot.loc[tf, cell_type] * 300  # Scale for visibility\n",
    "            sizes.append(size)\n",
    "            # Color based on mean expression\n",
    "            color = mean_expr_pivot.loc[tf, cell_type]\n",
    "            colors.append(color)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = ax.scatter(x_pos, y_pos, s=sizes, c=colors, cmap='Reds', alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Set labels and ticks\n",
    "    ax.set_xticks(range(len(cell_types)))\n",
    "    ax.set_xticklabels(cell_types, rotation=45, ha='right')\n",
    "    ax.set_yticks(range(len(mean_expr_pivot.index)))\n",
    "    ax.set_yticklabels(mean_expr_pivot.index)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Mean Expression Level', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title('TF Activity Across Cell Types\\n(Size: Detection Rate, Color: Expression Level)', fontsize=14)\n",
    "    ax.set_xlabel('Cell Types', fontsize=12)\n",
    "    ax.set_ylabel('TF Features', fontsize=12)\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/tf_activity_dotplot.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create interactive dot plot with Plotly\n",
    "    print(\"  ðŸ“Š Creating interactive TF activity plot...\")\n",
    "    \n",
    "    fig_plotly = go.Figure()\n",
    "    \n",
    "    for i, cell_type in enumerate(cell_types):\n",
    "        cell_data = dotplot_df[dotplot_df['CellType'] == cell_type]\n",
    "        \n",
    "        fig_plotly.add_trace(go.Scatter(\n",
    "            x=[cell_type] * len(cell_data),\n",
    "            y=cell_data['TF'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=cell_data['DetectionRate'] * 50,  # Scale for visibility\n",
    "                color=cell_data['MeanExpression'],\n",
    "                colorscale='Reds',\n",
    "                showscale=True if i == 0 else False,\n",
    "                colorbar=dict(title=\"Mean Expression\") if i == 0 else None,\n",
    "                line=dict(width=1, color='black')\n",
    "            ),\n",
    "            text=cell_data.apply(lambda row: f\"TF: {row['TF']}<br>\" +\n",
    "                                            f\"Cell Type: {row['CellType']}<br>\" +\n",
    "                                            f\"Mean Expr: {row['MeanExpression']:.3f}<br>\" +\n",
    "                                            f\"Detection Rate: {row['DetectionRate']:.1%}<br>\" +\n",
    "                                            f\"Significance: {row['Significance']:.2f}\", axis=1),\n",
    "            hovertemplate='%{text}<extra></extra>',\n",
    "            name=cell_type\n",
    "        ))\n",
    "    \n",
    "    fig_plotly.update_layout(\n",
    "        title='Interactive TF Activity Across Cell Types<br><sub>Size: Detection Rate, Color: Expression Level</sub>',\n",
    "        xaxis_title='Cell Types',\n",
    "        yaxis_title='TF Features',\n",
    "        height=max(600, len(all_top_tfs) * 25),\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig_plotly.write_html(f\"{save_dir}/tf_activity_dotplot_interactive.html\")\n",
    "    fig_plotly.show()\n",
    "    \n",
    "    print(f\"âœ… TF activity plots saved to {save_dir}/\")\n",
    "    \n",
    "    return dotplot_df, mean_expr_pivot, detection_rate_pivot\n",
    "\n",
    "# Create TF activity dot plots\n",
    "tf_activity_results = create_tf_activity_dotplot(\n",
    "    ml_data['X_train'], \n",
    "    ml_data['y_train_labels'], \n",
    "    feature_info,\n",
    "    cell_type_feature_analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Top TFs per Cell Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 10: TOP TFS PER CELL TYPE ANALYSIS\n",
    "# ================================================================================\n",
    "\n",
    "def create_top_tfs_per_celltype_analysis(cell_type_feature_analysis, X_train, y_train_labels, \n",
    "                                        save_dir=\"results/plots\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive analysis of top TFs for each cell type\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(\"ðŸ”„ Creating top TFs per cell type analysis...\")\n",
    "    \n",
    "    cell_types = list(cell_type_feature_analysis.keys())\n",
    "    n_cell_types = len(cell_types)\n",
    "    \n",
    "    # Create subplots for each cell type\n",
    "    fig, axes = plt.subplots(2, max(2, (n_cell_types + 1) // 2), figsize=(20, 12))\n",
    "    if n_cell_types == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    top_tfs_summary = {}\n",
    "    \n",
    "    for idx, cell_type in enumerate(cell_types):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        print(f\"  ðŸ“Š Analyzing {cell_type}...\")\n",
    "        \n",
    "        # Get top RNA features for this cell type\n",
    "        top_rna_features = cell_type_feature_analysis[cell_type]['top_rna'].head(10)\n",
    "        \n",
    "        if len(top_rna_features) == 0:\n",
    "            axes[idx].text(0.5, 0.5, f'No significant\\nRNA features\\nfor {cell_type}', \n",
    "                          ha='center', va='center', transform=axes[idx].transAxes)\n",
    "            axes[idx].set_title(cell_type)\n",
    "            continue\n",
    "        \n",
    "        # Create bar plot of top features\n",
    "        feature_names = top_rna_features['feature'].tolist()\n",
    "        scores = top_rna_features['score'].tolist()\n",
    "        \n",
    "        # Truncate long feature names for display\n",
    "        display_names = [name[:15] + '...' if len(name) > 15 else name for name in feature_names]\n",
    "        \n",
    "        bars = axes[idx].barh(range(len(display_names)), scores, alpha=0.7)\n",
    "        axes[idx].set_yticks(range(len(display_names)))\n",
    "        axes[idx].set_yticklabels(display_names, fontsize=8)\n",
    "        axes[idx].set_xlabel('F-Score', fontsize=10)\n",
    "        axes[idx].set_title(f'Top TFs for {cell_type}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, score) in enumerate(zip(bars, scores)):\n",
    "            axes[idx].text(score + max(scores) * 0.01, i, f'{score:.1f}', \n",
    "                          va='center', fontsize=8)\n",
    "        \n",
    "        # Store summary\n",
    "        top_tfs_summary[cell_type] = {\n",
    "            'features': feature_names,\n",
    "            'scores': scores,\n",
    "            'top_feature': feature_names[0] if feature_names else 'None',\n",
    "            'top_score': scores[0] if scores else 0\n",
    "        }\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(cell_types), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Top TF Features by Cell Type (F-Score Based)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/top_tfs_per_celltype.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create summary table\n",
    "    print(\"  ðŸ“Š Creating summary table...\")\n",
    "    \n",
    "    summary_data = []\n",
    "    for cell_type, data in top_tfs_summary.items():\n",
    "        for i, (feature, score) in enumerate(zip(data['features'][:5], data['scores'][:5])):\n",
    "            summary_data.append({\n",
    "                'CellType': cell_type,\n",
    "                'Rank': i + 1,\n",
    "                'TF_Feature': feature,\n",
    "                'F_Score': score,\n",
    "                'Is_Top': i == 0\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Create heatmap of top TF per cell type\n",
    "    top_tf_matrix = []\n",
    "    all_top_features = list(set([data['top_feature'] for data in top_tfs_summary.values() if data['top_feature'] != 'None']))\n",
    "    \n",
    "    for feature in all_top_features:\n",
    "        row = []\n",
    "        for cell_type in cell_types:\n",
    "            if feature == top_tfs_summary[cell_type]['top_feature']:\n",
    "                score = top_tfs_summary[cell_type]['top_score']\n",
    "            else:\n",
    "                # Get score for this feature in this cell type if available\n",
    "                cell_features = cell_type_feature_analysis[cell_type]['all_features']\n",
    "                if feature in cell_features['feature'].values:\n",
    "                    score = cell_features[cell_features['feature'] == feature]['score'].iloc[0]\n",
    "                else:\n",
    "                    score = 0\n",
    "            row.append(score)\n",
    "        top_tf_matrix.append(row)\n",
    "    \n",
    "    if top_tf_matrix:  # Only create heatmap if we have data\n",
    "        top_tf_df = pd.DataFrame(top_tf_matrix, columns=cell_types, index=all_top_features)\n",
    "        \n",
    "        plt.figure(figsize=(max(10, len(cell_types) * 1.2), max(6, len(all_top_features) * 0.5)))\n",
    "        sns.heatmap(top_tf_df, annot=True, fmt='.1f', cmap='YlOrRd', \n",
    "                   cbar_kws={'label': 'F-Score'})\n",
    "        plt.title('Top TF Features Across Cell Types', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Cell Types', fontsize=12)\n",
    "        plt.ylabel('Top TF Features', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_dir}/top_tfs_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Create interactive summary plot with Plotly\n",
    "    print(\"  ðŸ“Š Creating interactive summary visualization...\")\n",
    "    \n",
    "    # Prepare data for interactive plot\n",
    "    plot_data = []\n",
    "    for cell_type, data in top_tfs_summary.items():\n",
    "        for i, (feature, score) in enumerate(zip(data['features'][:10], data['scores'][:10])):\n",
    "            plot_data.append({\n",
    "                'CellType': cell_type,\n",
    "                'TF': feature,\n",
    "                'Score': score,\n",
    "                'Rank': i + 1\n",
    "            })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Create interactive bar plot\n",
    "    fig_interactive = px.bar(plot_df, x='TF', y='Score', color='CellType',\n",
    "                           title='Top TF Features by Cell Type (Interactive)',\n",
    "                           labels={'Score': 'F-Score', 'TF': 'TF Features'},\n",
    "                           hover_data=['Rank'])\n",
    "    \n",
    "    fig_interactive.update_layout(\n",
    "        height=600,\n",
    "        xaxis_tickangle=-45,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig_interactive.write_html(f\"{save_dir}/top_tfs_interactive.html\")\n",
    "    fig_interactive.show()\n",
    "    \n",
    "    print(f\"âœ… Top TFs analysis saved to {save_dir}/\")\n",
    "    \n",
    "    return top_tfs_summary, summary_df\n",
    "\n",
    "# Create top TFs per cell type analysis\n",
    "top_tfs_results = create_top_tfs_per_celltype_analysis(\n",
    "    cell_type_feature_analysis,\n",
    "    ml_data['X_train'],\n",
    "    ml_data['y_train_labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Model Timing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 11: MODEL TIMING ANALYSIS\n",
    "# ================================================================================\n",
    "\n",
    "def create_timing_analysis(timing_results, model_results, save_dir=\"results/plots\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive timing analysis for all models\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(\"ðŸ”„ Creating model timing analysis...\")\n",
    "    \n",
    "    # Extract timing data\n",
    "    model_names = list(timing_results.keys())\n",
    "    training_times = [timing_results[m]['training_time'] for m in model_names]\n",
    "    prediction_times = [timing_results[m]['prediction_time'] for m in model_names]\n",
    "    \n",
    "    # Extract performance data\n",
    "    test_accuracies = [model_results[m]['test_score'] for m in model_names]\n",
    "    \n",
    "    # Create comprehensive timing plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Training Time Comparison\n",
    "    bars1 = axes[0,0].bar(model_names, training_times, alpha=0.8, color='lightblue', edgecolor='navy')\n",
    "    axes[0,0].set_ylabel('Training Time (seconds)')\n",
    "    axes[0,0].set_title('Model Training Time Comparison')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, time in zip(bars1, training_times):\n",
    "        height = bar.get_height()\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.02*max(training_times),\n",
    "                      f'{time:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Prediction Time Comparison\n",
    "    bars2 = axes[0,1].bar(model_names, prediction_times, alpha=0.8, color='lightgreen', edgecolor='darkgreen')\n",
    "    axes[0,1].set_ylabel('Prediction Time (seconds)')\n",
    "    axes[0,1].set_title('Model Prediction Time Comparison')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, time in zip(bars2, prediction_times):\n",
    "        height = bar.get_height()\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2., height + 0.02*max(prediction_times),\n",
    "                      f'{time:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Performance vs Training Time\n",
    "    scatter1 = axes[1,0].scatter(training_times, test_accuracies, s=100, alpha=0.7, \n",
    "                                c=range(len(model_names)), cmap='viridis')\n",
    "    \n",
    "    # Add model labels\n",
    "    for i, model in enumerate(model_names):\n",
    "        axes[1,0].set_title('Performance vs Training Time')\n    axes[1,0].grid(True, alpha=0.3)\n    \n    # Plot 4: Efficiency Score (Accuracy/Time)\n    efficiency_scores = [acc/time if time > 0 else 0 for acc, time in zip(test_accuracies, training_times)]\n    bars4 = axes[1,1].bar(model_names, efficiency_scores, alpha=0.8, color='orange', edgecolor='darkorange')\n    axes[1,1].set_ylabel('Efficiency Score (Accuracy/Training Time)')\n    axes[1,1].set_title('Model Efficiency Comparison')\n    axes[1,1].tick_params(axis='x', rotation=45)\n    axes[1,1].grid(True, alpha=0.3)\n    \n    # Add value labels\n    for bar, score in zip(bars4, efficiency_scores):\n        height = bar.get_height()\n        axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.02*max(efficiency_scores),\n                      f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n    \n    plt.tight_layout()\n    plt.savefig(f\"{save_dir}/model_timing_analysis.png\", dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # Create detailed timing summary table\n    timing_summary = pd.DataFrame({\n        'Model': model_names,\n        'Training_Time_s': training_times,\n        'Prediction_Time_s': prediction_times,\n        'Test_Accuracy': test_accuracies,\n        'Efficiency_Score': efficiency_scores,\n        'Speed_Rank': pd.Series(training_times).rank().astype(int),\n        'Accuracy_Rank': pd.Series(test_accuracies).rank(ascending=False).astype(int),\n        'Efficiency_Rank': pd.Series(efficiency_scores).rank(ascending=False).astype(int)\n    })\n    \n    print(\"\\nðŸ“Š Model Timing Summary:\")\n    print(timing_summary.round(4))\n    \n    # Create interactive timing plot with Plotly\n    print(\"  ðŸ“Š Creating interactive timing visualization...\")\n    \n    fig_timing = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=['Training Time', 'Prediction Time', 'Performance vs Time', 'Efficiency Score'],\n        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n    )\n    \n    # Training time bar chart\n    fig_timing.add_trace(\n        go.Bar(x=model_names, y=training_times, name='Training Time', \n               marker_color='lightblue', text=[f'{t:.2f}s' for t in training_times], textposition='outside'),\n        row=1, col=1\n    )\n    \n    # Prediction time bar chart\n    fig_timing.add_trace(\n        go.Bar(x=model_names, y=prediction_times, name='Prediction Time',\n               marker_color='lightgreen', text=[f'{t:.3f}s' for t in prediction_times], textposition='outside'),\n        row=1, col=2\n    )\n    \n    # Scatter plot: Performance vs Time\n    fig_timing.add_trace(\n        go.Scatter(x=training_times, y=test_accuracies, mode='markers+text',\n                  text=model_names, textposition='top center',\n                  marker=dict(size=10, color='red'), name='Performance vs Time'),\n        row=2, col=1\n    )\n    \n    # Efficiency score bar chart\n    fig_timing.add_trace(\n        go.Bar(x=model_names, y=efficiency_scores, name='Efficiency Score',\n               marker_color='orange', text=[f'{e:.3f}' for e in efficiency_scores], textposition='outside'),\n        row=2, col=2\n    )\n    \n    fig_timing.update_layout(\n        height=800,\n        title_text=\"Model Timing Analysis Dashboard\",\n        showlegend=False\n    )\n    \n    # Update x-axis labels\n    fig_timing.update_xaxes(tickangle=45)\n    \n    fig_timing.write_html(f\"{save_dir}/model_timing_interactive.html\")\n    fig_timing.show()\n    \n    print(f\"âœ… Timing analysis saved to {save_dir}/\")\n    \n    return timing_summary\n\n# Create timing analysis\ntiming_analysis_results = create_timing_analysis(timing_results, model_results)"annotate(model, (training_times[i], test_accuracies[i]), \n",
    "                          xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[1,0].set_xlabel('Training Time (seconds)')\n",
    "    axes[1,0].set_ylabel('Test Accuracy')\n",
    "    axes[1,0].  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 6: MODEL PERFORMANCE VISUALIZATION\n",
    "# ================================================================================\n",
    "\n",
    "def create_model_performance_plots(model_results, timing_results, save_dir=\"results/plots\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive model performance visualizations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"ðŸ”„ Creating model performance visualizations...\")\n",
    "    \n",
    "    # 1. Model Performance Comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Extract metrics for plotting\n",
    "    model_names = list(model_results.keys())\n",
    "    train_scores = [model_results[m]['train_score'] for m in model_names]\n",
    "    test_scores = [model_results[m]['test_score'] for m in model_names]\n",
    "    cv_means = [model_results[m]['cv_mean'] for m in model_names]\n",
    "    cv_stds = [model_results[m]['cv_std'] for m in model_names]\n",
    "    overfitting_scores = [model_results[m]['overfitting_score'] for m in model_names]\n",
    "    \n",
    "    # Plot 1: Train vs Test Accuracy\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0,0].bar(x - width/2, train_scores, width, label='Train', alpha=0.8)\n",
    "    axes[0,0].bar(x + width/2, test_scores, width, label='Test', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Models')\n",
    "    axes[0,0].set_ylabel('Accuracy')\n",
    "    axes[0,0].set_title('Train vs Test Accuracy Comparison')\n",
    "    axes[0,0].set_xticks(x)\n",
    "    axes[0,0].set_xticklabels(model_names, rotation=45)\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Cross-Validation Scores\n",
    "    axes[0,1].errorbar(x, cv_means, yerr=cv_stds, fmt='o-', capsize=5, capthick=2)\n",
    "    axes[0,1].set_xlabel('Models')\n",
    "    axes[0,1].set_ylabel('CV Accuracy')\n",
    "    axes[0,1].set_title('Cross-Validation Performance')\n",
    "    axes[0,1].set_xticks(x)\n",
    "    axes[0,1].set_xticklabels(model_names, rotation=45)\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Overfitting Analysis\n",
    "    colors = ['red' if score > 0.1 else 'orange' if score > 0.05 else 'green' for score in overfitting_scores]\n",
    "    bars = axes[1,0].bar(x, overfitting_scores, color=colors, alpha=0.7)\n",
    "    axes[1,0].axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='High Overfitting')\n",
    "    axes[1,0].axhline(y=0.05, color='orange', linestyle='--', alpha=0.5, label='Moderate Overfitting')\n",
    "    axes[1,0].axhline(y=-0.05, color='blue', linestyle='--', alpha=0.5, label='Potential Underfitting')\n",
    "    axes[1,0].set_xlabel('Models')\n",
    "    axes[1,0].set_ylabel('Train - Test Accuracy')\n",
    "    axes[1,0].set_title('Overfitting Analysis')\n",
    "    axes[1,0].set_xticks(x)\n",
    "    axes[1,0].set_xticklabels(model_names, rotation=45)\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Training Time Comparison\n",
    "    training_times = [timing_results[m]['training_time'] for m in model_names]\n",
    "    bars = axes[1,1].bar(x, training_times, alpha=0.8, color='skyblue')\n",
    "    axes[1,1].set_xlabel('Models')\n",
    "    axes[1,1].set_ylabel('Training Time (seconds)')\n",
    "    axes[1,1].set_title('Model Training Time Comparison')\n",
    "    axes[1,1].set_xticks(x)\n",
    "    axes[1,1].set_xticklabels(model_names, rotation=45)\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time in zip(bars, training_times):\n",
    "        height = bar.get_height()\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.05*max(training_times),\n",
    "                      f'{time:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/model_performance_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Learning Curves for Overfitting Analysis\n",
    "    n_models = len(model_results)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (model_name, result) in enumerate(model_results.items()):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        train_sizes = result['train_sizes']\n",
    "        train_scores_mean = np.mean(result['train_scores_lc'], axis=1)\n",
    "        train_scores_std = np.std(result['train_scores_lc'], axis=1)\n",
    "        val_scores_mean = np.mean(result['val_scores_lc'], axis=1)\n",
    "        val_scores_std = np.std(result['val_scores_lc'], axis=1)\n",
    "        \n",
    "        axes[idx].plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training')\n",
    "        axes[idx].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                              train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
    "        \n",
    "        axes[idx].plot(train_sizes, val_scores_mean, 'o-', color='red', label='Validation')\n",
    "        axes[idx].fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                              val_scores_mean + val_scores_std, alpha=0.1, color='red')\n",
    "        \n",
    "        axes[idx].set_xlabel('Training Size')\n",
    "        axes[idx].set_ylabel('Accuracy')\n",
    "        axes[idx].set_title(f'{model_name} Learning Curve\\n{result[\"overfitting_status\"]}')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_models, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/learning_curves.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Performance plots saved to {save_dir}/\")\n",
    "\n",
    "# Create performance visualizations\n",
    "create_model_performance_plots(model_results, timing_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Importance Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 7: FEATURE IMPORTANCE HEATMAPS\n",
    "# ================================================================================\n",
    "\n",
    "def create_feature_importance_heatmaps(feature_importance_results, cell_type_feature_analysis, \n",
    "                                     label_encoder, save_dir=\"results/plots\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive feature importance heatmaps\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(\"ðŸ”„ Creating feature importance heatmaps...\")\n",
    "    \n",
    "    # 1. Top 20 Features Across All Models Heatmap\n",
    "    print(\"  ðŸ“Š Creating cross-model feature importance heatmap...\")\n",
    "    \n",
    "    # Collect top features from each model\n",
    "    all_top_features = set()\n",
    "    for model_name, features in feature_importance_results.items():\n",
    "        top_features = features['top_features']['feature'].head(20).tolist()\n",
    "        all_top_features.update(top_features)\n",
    "    \n",
    "    # Create importance matrix\n",
    "    importance_matrix = []\n",
    "    feature_list = list(all_top_features)\n",
    "    model_names = list(feature_importance_results.keys())\n",
    "    \n",
    "    for feature in feature_list:\n",
    "        row = []\n",
    "        for model_name in model_names:\n",
    "            feature_df = feature_importance_results[model_name]['all_features']\n",
    "            if feature in feature_df['feature'].values:\n",
    "                importance = feature_df[feature_df['feature'] == feature]['importance'].iloc[0]\n",
    "            else:\n",
    "                importance = 0\n",
    "            row.append(importance)\n",
    "        importance_matrix.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame(importance_matrix, columns=model_names, index=feature_list)\n",
    "    \n",
    "    # Normalize by column (model) for better visualization\n",
    "    importance_df_norm = importance_df.div(importance_df.max(axis=0), axis=1)\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, max(8, len(feature_list) * 0.3)))\n",
    "    sns.heatmap(importance_df_norm, annot=False, cmap='viridis', cbar_kws={'label': 'Normalized Importance'})\n",
    "    plt.title('Top 20 Features Importance Across Models')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/feature_importance_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Cell Type Specific Feature Heatmap\n",
    "    print(\"  ðŸ“Š Creating cell-type specific feature heatmap...\")\n",
    "    \n",
    "    # Collect top features per cell type\n",
    "    cell_type_matrix = []\n",
    "    all_cell_type_features = set()\n",
    "    cell_types = list(cell_type_feature_analysis.keys())\n",
    "    \n",
    "    # Get union of top features across cell types\n",
    "    for cell_type in cell_types:\n",
    "        top_features = cell_type_feature_analysis[cell_type]['top_features']['feature'].head(15).tolist()\n",
    "        all_cell_type_features.update(top_features)\n",
    "    \n",
    "    feature_list_ct = list(all_cell_type_features)\n",
    "    \n",
    "    for feature in feature_list_ct:\n",
    "        row = []\n",
    "        for cell_type in cell_types:\n",
    "            feature_df = cell_type_feature_analysis[cell_type]['all_features']\n",
    "            if feature in feature_df['feature'].values:\n",
    "                score = feature_df[feature_df['feature'] == feature]['score'].iloc[0]\n",
    "            else:\n",
    "                score = 0\n",
    "            row.append(score)\n",
    "        cell_type_matrix.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    cell_type_df = pd.DataFrame(cell_type_matrix, columns=cell_types, index=feature_list_ct)\n",
    "    \n",
    "    # Normalize by column for better visualization\n",
    "    cell_type_df_norm = cell_type_df.div(cell_type_df.max(axis=0), axis=1)\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(max(10, len(cell_types) * 1.5), max(8, len(feature_list_ct) * 0.3)))\n",
    "    sns.heatmap(cell_type_df_norm, annot=False, cmap='plasma', \n",
    "                cbar_kws={'label': 'Normalized F-Score'})\n",
    "    plt.title('Top Features by Cell Type (F-Score Based)')\n",
    "    plt.xlabel('Cell Types')\n",
    "    plt.ylabel('Features')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/celltype_feature_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. RNA vs ATAC Feature Distribution\n",
    "    print(\"  ðŸ“Š Creating RNA vs ATAC feature distribution plot...\")\n",
    "    \n",
    "    # Count RNA vs ATAC features in top features for each model\n",
    "    rna_atac_counts = []\n",
    "    for model_name, features in feature_importance_results.items():\n",
    "        top_20 = features['top_features'].head(20)\n",
    "        rna_count = sum(top_20['feature_type'] == 'RNA')\n",
    "        atac_count = sum(top_20['feature_type'] == 'ATAC')\n",
    "        rna_atac_counts.append([model_name, rna_count, atac_count])\n",
    "    \n",
    "    rna_atac_df = pd.DataFrame(rna_atac_counts, columns=['Model', 'RNA', 'ATAC'])\n",
    "    \n",
    "    # Create stacked bar plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(rna_atac_df))\n",
    "    width = 0.6\n",
    "    \n",
    "    ax.bar(x, rna_atac_df['RNA'], width, label='RNA Features', alpha=0.8, color='lightcoral')\n",
    "    ax.bar(x, rna_atac_df['ATAC'], width, bottom=rna_atac_df['RNA'], \n",
    "           label='ATAC Features', alpha=0.8, color='lightblue')\n",
    "    \n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Number of Features in Top 20')\n",
    "    ax.set_title('RNA vs ATAC Feature Distribution in Top 20 Features')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(rna_atac_df['Model'], rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (rna, atac) in enumerate(zip(rna_atac_df['RNA'], rna_atac_df['ATAC'])):\n",
    "        ax.text(i, rna/2, str(rna), ha='center', va='center', fontweight='bold')\n",
    "        ax.text(i, rna + atac/2, str(atac), ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/rna_atac_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Feature importance heatmaps saved to {save_dir}/\")\n",
    "    \n",
    "    return importance_df, cell_type_df, rna_atac_df\n",
    "\n",
    "# Create feature importance heatmaps\n",
    "importance_matrices = create_feature_importance_heatmaps(\n",
    "    feature_importance_results, \n",
    "    cell_type_feature_analysis, \n",
    "    ml_data['label_encoder']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: TF-Cell Type Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 8: TF-CELL TYPE NETWORK ANALYSIS\n",
    "# ================================================================================\n",
    "\n",
    "def create_tf_celltype_network(X_train, y_train_labels, feature_info, top_k=10, \n",
    "                              correlation_threshold=0.3, save_dir=\"results/plots\"):\n",
    "    \"\"\"\n",
    "    Create TF-Cell Type network visualization based on feature correlations and cell-type associations\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(\"ðŸ”„ Creating TF-Cell Type network...\")\n",
    "    \n",
    "    # Filter for potential transcription factors (genes starting with common TF patterns)\n",
    "    # This is a simplified approach - in reality, you'd use a TF database\n",
    "    tf_patterns = ['FOXP', 'GATA', 'TBX', 'SOX', 'PAX', 'HOX', 'IRF', 'STAT', 'JUN', 'FOS',\n",
    "                  'MYC', 'TP53', 'NFKB', 'CREB', 'ETS', 'RUNX', 'SMAD', 'TCF', 'LEF', 'HIF']\n",
    "    \n",
    "    # Get RNA features that might be TFs\n",
    "    rna_features = [f for f in X_train.columns if feature_type_map.get(f) == 'RNA']\n",
    "    potential_tfs = []\n",
    "    \n",
    "    for feature in rna_features:\n",
    "        for pattern in tf_patterns:\n",
    "            if pattern in feature.upper():\n",
    "                potential_tfs.append(feature)\n",
    "                break\n",
    "    \n",
    "    # If no TFs found with patterns, use top variable RNA features\n",
    "    if len(potential_tfs) < 10:\n",
    "        print(\"  âš ï¸ Limited TFs found with patterns, using top variable RNA features\")\n",
    "        # Calculate variance for RNA features\n",
    "        rna_data = X_train[rna_features]\n",
    "        feature_variance = rna_data.var().sort_values(ascending=False)\n",
    "        potential_tfs = feature_variance.head(30).index.tolist()\n",
    "    \n",
    "    print(f\"  ðŸ“Š Found {len(potential_tfs)} potential TF features\")\n",
    "    \n",
    "    # Calculate mean expression of TFs per cell type\n",
    "    cell_types = y_train_labels.unique()\n",
    "    tf_celltype_matrix = []\n",
    "    \n",
    "    for tf in potential_tfs:\n",
    "        if tf in X_train.columns:\n",
    "            tf_row = []\n",
    "            for cell_type in cell_types:\n",
    "                mask = y_train_labels == cell_type\n",
    "                mean_expr = X_train.loc[mask, tf].mean()\n",
    "                tf_row.append(mean_expr)\n",
    "            tf_celltype_matrix.append(tf_row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    tf_celltype_df = pd.DataFrame(tf_celltype_matrix, \n",
    "                                 columns=cell_types, \n",
    "                                 index=potential_tfs[:len(tf_celltype_matrix)])\n",
    "    \n",
    "    # Calculate correlations between TFs\n",
    "    tf_data = X_train[tf_celltype_df.index]\n",
    "    tf_correlations = tf_data.corr()\n",
    "    \n",
    "    # Create network graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add TF nodes\n",
    "    for tf in tf_celltype_df.index:\n",
    "        G.add_node(tf, node_type='TF', size=20)\n",
    "    \n",
    "    # Add cell type nodes\n",
    "    for cell_type in cell_types:\n",
    "        G.add_node(cell_type, node_type='CellType', size=30)\n",
    "    \n",
    "    # Add edges between TFs based on correlation\n",
    "    for i, tf1 in enumerate(tf_celltype_df.index):\n",
    "        for j, tf2 in enumerate(tf_celltype_df.index):\n",
    "            if i < j:  # Avoid duplicate edges\n",
    "                corr = tf_correlations.loc[tf1, tf2]\n",
    "                if abs(corr) > correlation_threshold:\n",
    "                    G.add_edge(tf1, tf2, weight=abs(corr), edge_type='TF-TF')\n",
    "    \n",
    "    # Add edges between TFs and cell types based on expression levels\n",
    "    for tf in tf_celltype_df.index:\n",
    "        # Find top cell types for this TF\n",
    "        tf_expr = tf_celltype_df.loc[tf]\n",
    "        top_celltypes = tf_expr.nlargest(min(3, len(cell_types)))  # Top 3 cell types\n",
    "        \n",
    "        for cell_type in top_celltypes.index:\n",
    "            if top_celltypes[cell_type] > tf_expr.mean() + tf_expr.std():  # Above mean + 1 std\n",
    "                G.add_edge(tf, cell_type, weight=top_celltypes[cell_type], edge_type='TF-CellType')\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Position nodes using spring layout\n",
    "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "    \n",
    "    # Draw nodes\n",
    "    tf_nodes = [node for node, data in G.nodes(data=True) if data['node_type'] == 'TF']\n",
    "    celltype_nodes = [node for node, data in G.nodes(data=True) if data['node_type'] == 'CellType']\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=tf_nodes, node_color='lightblue', \n",
    "                          node_size=300, alpha=0.8, label='TFs')\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=celltype_nodes, node_color='lightcoral', \n",
    "                          node_size=500, alpha=0.8, label='Cell Types')\n",
    "    \n",
    "    # Draw edges\n",
    "    tf_tf_edges = [(u, v) for u, v, data in G.edges(data=True) if data['edge_type'] == 'TF-TF']\n",
    "    tf_ct_edges = [(u, v) for u, v, data in G.edges(data=True) if data['edge_type'] == 'TF-CellType']\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, edgelist=tf_tf_edges, edge_color='gray', \n",
    "                          alpha=0.5, width=1, style='dashed')\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=tf_ct_edges, edge_color='red', \n",
    "                          alpha=0.7, width=2)\n",
    "    \n",
    "    # Draw labels\n",
    "    labels = {}\n",
    "    for node in G.nodes():\n",
    "        if len(node) > 10:\n",
    "            labels[node] = node[:10] + '...'\n",
    "        else:\n",
    "            labels[node] = node\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "    \n",
    "    plt.title('TF-Cell Type Network\\n(Blue: TFs, Red: Cell Types, Dashed: TF-TF correlations, Solid: TF-CellType associations)', 
              fontsize=12)
    plt.legend()
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(f"{save_dir}/tf_celltype_network.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    # Create interactive network with Plotly
    print("  ðŸ“Š Creating interactive network visualization...")
    
    # Prepare data for Plotly
    edge_x = []
    edge_y = []
    edge_info = []
    
    for edge in G.edges(data=True):\n        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])
        edge_info.append(f"{edge[0]} - {edge[1]}: {edge[2]['edge_type']}")
    
    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=1, color='gray'),
                           hoverinfo='none', mode='lines')
    
    node_x = []
    node_y = []
    node_text = []
    node_color = []
    node_size = []
    
    for node in G.nodes(data=True):
        x, y = pos[node[0]]
        node_x.append(x)
        node_y.append(y)
        node_text.append(node[0])
        
        if node[1]['node_type'] == 'TF':
            node_color.append('lightblue')
            node_size.append(20)
        else:
            node_color.append('lightcoral')
            node_size.append(30)
    
    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers+text',
                           hoverinfo='text', text=node_text, textposition="middle center",
                           marker=dict(size=node_size, color=node_color, line=dict(width=2)))
    
    fig = go.Figure(data=[edge_trace, node_trace],
                   layout=go.Layout(title='Interactive TF-Cell Type Network',
                                   titlefont_size=16,
                                   showlegend=False,
                                   hovermode='closest',
                                   margin=dict(b=20,l=5,r=5,t=40),
                                   annotations=[ dict(text="Blue: TFs, Red: Cell Types",
                                                     showarrow=False,
                                                     xref="paper", yref="paper",
                                                     x=0.005, y=-0.002,
                                                     xanchor="left", yanchor="bottom",
                                                     font=dict(size=12))],
                                   xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                                   yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))
    
    fig.write_html(f"{save_dir}/tf_celltype_network_interactive.html")
    fig.show()
    
    print(f"âœ… TF-Cell Type network saved to {save_dir}/")
    
    return G, tf_celltype_df

# Create TF-Cell Type network
network_results = create_tf_celltype_network(
    ml_data['X_train'], 
    ml_data['y_train_labels'], 
    feature_info
){
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell-Type Classification Model Validation & Visualization Pipeline\n",
    "## Preliminary result_v2.ipynb\n",
    "\n",
    "**Authors:** Research Team  \n",
    "**Date:** 2025  \n",
    "**Description:** Comprehensive validation and visualization pipeline for multi-modal (RNA + ATAC) cell-type classification models\n",
    "\n",
    "### Pipeline Overview:\n",
    "1. **Data Loading & Validation** - Load processed data from R pipeline\n",
    "2. **Model Building & Training** - Multiple ML models with overfitting checks\n",
    "3. **Model Validation** - Performance metrics and cross-validation\n",
    "4. **Feature Analysis** - Top features identification and analysis\n",
    "5. **Visualization Suite** - TF networks, heatmaps, dotplots, timing analysis\n",
    "6. **Results Export** - Save all results for downstream analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup & Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 1: LIBRARY IMPORTS & ENVIRONMENT SETUP\n",
    "# ================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score, \n",
    "    learning_curve, \n",
    "    validation_curve,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Network and Graph Libraries\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Plotting Libraries\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ðŸ“… Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Loading & Initial Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 2: DATA LOADING FROM R PIPELINE OUTPUTS\n",
    "# ================================================================================\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    \"\"\"\n",
    "    Load all preprocessed data from R pipeline outputs\n",
    "    Expected files from python_ready_data directory:\n",
    "    - combined_features.csv\n",
    "    - cell_labels.csv  \n",
    "    - data_splits.csv\n",
    "    - feature_info.csv\n",
    "    - data_summary.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Loading preprocessed data from R pipeline...\")\n",
    "    \n",
    "    # Define data directory\n",
    "    data_dir = Path(\"python_ready_data\")\n",
    "    \n",
    "    if not data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Data directory {data_dir} not found. Please run R preprocessing first.\")\n",
    "    \n",
    "    # Load main datasets\n",
    "    try:\n",
    "        # Combined feature matrix (cells x features)\n",
    "        features_df = pd.read_csv(data_dir / \"combined_features.csv\", index_col=0)\n",
    "        print(f\"âœ… Features loaded: {features_df.shape[0]} cells Ã— {features_df.shape[1]} features\")\n",
    "        \n",
    "        # Cell type labels\n",
    "        labels_df = pd.read_csv(data_dir / \"cell_labels.csv\")\n",
    "        print(f\"âœ… Labels loaded: {len(labels_df)} cells with {labels_df['cell_type'].nunique()} unique cell types\")\n",
    "        \n",
    "        # Train/test splits\n",
    "        splits_df = pd.read_csv(data_dir / \"data_splits.csv\")\n",
    "        print(f\"âœ… Splits loaded: {sum(splits_df['split'] == 'train')} train, {sum(splits_df['split'] == 'test')} test\")\n",
    "        \n",
    "        # Feature information\n",
    "        feature_info_df = pd.read_csv(data_dir / \"feature_info.csv\")\n",
    "        print(f\"âœ… Feature info loaded: {len(feature_info_df)} features\")\n",
    "        \n",
    "        # Data summary\n",
    "        summary_df = pd.read_csv(data_dir / \"data_summary.csv\")\n",
    "        print(f\"âœ… Summary stats loaded\")\n",
    "        \n",
    "        # Optional: Load individual modality data\n",
    "        rna_features_df = None\n",
    "        atac_features_df = None\n",
    "        \n",
    "        if (data_dir / \"rna_features_2000.csv\").exists():\n",
    "            rna_features_df = pd.read_csv(data_dir / \"rna_features_2000.csv\", index_col=0)\n",
    "            print(f\"âœ… RNA features loaded: {rna_features_df.shape}\")\n",
    "            \n",
    "        if (data_dir / \"atac_features_5000.csv\").exists():\n",
    "            atac_features_df = pd.read_csv(data_dir / \"atac_features_5000.csv\", index_col=0)\n",
    "            print(f\"âœ… ATAC features loaded: {atac_features_df.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'features': features_df,\n",
    "            'labels': labels_df,\n",
    "            'splits': splits_df,\n",
    "            'feature_info': feature_info_df,\n",
    "            'summary': summary_df,\n",
    "            'rna_features': rna_features_df,\n",
    "            'atac_features': atac_features_df\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading data: {str(e)}\")\n",
    "        print(\"Available files in directory:\")\n",
    "        for file in data_dir.glob(\"*.csv\"):\n",
    "            print(f\"  - {file.name}\")\n",
    "        raise\n",
    "\n",
    "# Load the data\n",
    "data_dict = load_preprocessed_data()\n",
    "\n",
    "# Extract main components for easier access\n",
    "X = data_dict['features']\n",
    "y_df = data_dict['labels']\n",
    "splits_df = data_dict['splits']\n",
    "feature_info = data_dict['feature_info']\n",
    "\n",
    "print(\"\\nðŸ“Š Data Loading Summary:\")\n",
    "print(f\"  - Total cells: {X.shape[0]:,}\")\n",
    "print(f\"  - Total features: {X.shape[1]:,}\")\n",
    "print(f\"  - Cell types: {', '.join(y_df['cell_type'].unique())}\")\n",
    "print(f\"  - RNA features: {sum(feature_info['feature_type'] == 'RNA'):,}\")\n",
    "print(f\"  - ATAC features: {sum(feature_info['feature_type'] == 'ATAC'):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preparation & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 3: DATA PREPARATION & FEATURE ENGINEERING\n",
    "# ================================================================================\n",
    "\n",
    "def prepare_ml_data(X, y_df, splits_df, feature_info):\n",
    "    \"\"\"\n",
    "    Prepare data for machine learning with proper scaling and encoding\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Preparing data for machine learning...\")\n",
    "    \n",
    "    # Align data\n",
    "    common_cells = list(set(X.index) & set(y_df['cell_id']) & set(splits_df['cell_id']))\n",
    "    print(f\"âœ… Found {len(common_cells)} common cells across all datasets\")\n",
    "    \n",
    "    # Filter and align\n",
    "    X_aligned = X.loc[common_cells]\n",
    "    y_aligned = y_df[y_df['cell_id'].isin(common_cells)].set_index('cell_id').loc[common_cells]\n",
    "    splits_aligned = splits_df[splits_df['cell_id'].isin(common_cells)].set_index('cell_id').loc[common_cells]\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_aligned['cell_type'])\n",
    "    \n",
    "    # Create train/test splits\n",
    "    train_mask = splits_aligned['split'] == 'train'\n",
    "    test_mask = splits_aligned['split'] == 'test'\n",
    "    \n",
    "    X_train = X_aligned[train_mask]\n",
    "    X_test = X_aligned[test_mask]\n",
    "    y_train = y_encoded[train_mask]\n",
    "    y_test = y_encoded[test_mask]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrames for easier handling\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    print(f\"âœ… Training set: {X_train_scaled.shape[0]} cells\")\n",
    "    print(f\"âœ… Test set: {X_test_scaled.shape[0]} cells\")\n",
    "    print(f\"âœ… Features scaled using StandardScaler\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'label_encoder': label_encoder,\n",
    "        'scaler': scaler,\n",
    "        'y_train_labels': y_aligned['cell_type'][train_mask],\n",
    "        'y_test_labels': y_aligned['cell_type'][test_mask]\n",
    "    }\n",
    "\n",
    "# Prepare the data\n",
    "ml_data = prepare_ml_data(X, y_df, splits_df, feature_info)\n",
    "\n",
    "# Create feature type mapping for later analysis\n",
    "feature_type_map = dict(zip(feature_info['feature_name'], feature_info['feature_type']))\n",
    "rna_features = [f for f in ml_data['X_train'].columns if feature_type_map.get(f) == 'RNA']\n",
    "atac_features = [f for f in ml_data['X_train'].columns if feature_type_map.get(f) == 'ATAC']\n",
    "\n",
    "print(f\"\\nðŸ“‹ Feature Summary:\")\n",
    "print(f\"  - RNA features: {len(rna_features):,}\")\n",
    "print(f\"  - ATAC features: {len(atac_features):,}\")\n",
    "print(f\"  - Cell types: {list(ml_data['label_encoder'].classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Building & Training with Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 4: MODEL BUILDING & TRAINING WITH OVERFITTING DETECTION\n",
    "# ================================================================================\n",
    "\n",
    "def train_models_with_validation(X_train, y_train, X_test, y_test, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Train multiple models with comprehensive validation and overfitting analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Training multiple models with validation...\")\n",
    "    \n",
    "    # Define models to train\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            max_depth=10, \n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(\n",
    "            n_estimators=100, \n",
    "            max_depth=6, \n",
    "            random_state=42\n",
    "        ),\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            max_iter=1000, \n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'SVM': SVC(\n",
    "            kernel='rbf', \n",
    "            probability=True, \n",
    "            random_state=42\n",
    "        ),\n",
    "        'Neural Network': MLPClassifier(\n",
    "            hidden_layer_sizes=(100, 50), \n",
    "            max_iter=500, \n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    model_timings = {}\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nðŸ”„ Training {model_name}...\")\n",
    "        \n",
    "        # Time the training\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        model_timings[model_name] = {\n",
    "            'training_time': training_time,\n",
    "            'prediction_time': 0\n",
    "        }\n",
    "        \n",
    "        # Make predictions\n",
    "        pred_start = time.time()\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        model_timings[model_name]['prediction_time'] = time.time() - pred_start\n",
    "        \n",
    "        # Calculate scores\n",
    "        train_score = accuracy_score(y_train, y_train_pred)\n",
    "        test_score = accuracy_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        # Learning curves for overfitting analysis\n",
    "        train_sizes, train_scores_lc, val_scores_lc = learning_curve(\n",
    "            model, X_train, y_train, cv=3, n_jobs=-1,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "            scoring='accuracy'\n",
    "        )\n",
    "        \n",
    "        # Calculate overfitting metrics\n",
    "        overfitting_score = train_score - test_score\n",
    "        cv_std = cv_scores.std()\n",
    "        \n",
    "        # Determine overfitting status\n",
    "        if overfitting_score > 0.1:\n",
    "            overfitting_status = \"High Overfitting\"\n",
    "        elif overfitting_score > 0.05:\n",
    "            overfitting_status = \"Moderate Overfitting\"\n",
    "        elif overfitting_score < -0.05:\n",
    "            overfitting_status = \"Potential Underfitting\"\n",
    "        else:\n",
    "            overfitting_status = \"Good Fit\"\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'model': model,\n",
    "            'train_score': train_score,\n",
    "            'test_score': test_score,\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_std,\n",
    "            'overfitting_score': overfitting_score,\n",
    "            'overfitting_status': overfitting_status,\n",
    "            'train_sizes': train_sizes,\n",
    "            'train_scores_lc': train_scores_lc,\n",
    "            'val_scores_lc': val_scores_lc,\n",
    "            'y_train_pred': y_train_pred,\n",
    "            'y_test_pred': y_test_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… {model_name}:\")\n",
    "        print(f\"     - Training Accuracy: {train_score:.4f}\")\n",
    "        print(f\"     - Test Accuracy: {test_score:.4f}\")\n",
    "        print(f\"     - CV Score: {cv_scores.mean():.4f} Â± {cv_std:.4f}\")\n",
    "        print(f\"     - Overfitting: {overfitting_status} ({overfitting_score:.4f})\")\n",
    "        print(f\"     - Training Time: {training_time:.2f}s\")\n",
    "    \n",
    "    return results, model_timings\n",
    "\n",
    "# Train all models\n",
    "model_results, timing_results = train_models_with_validation(\n",
    "    ml_data['X_train'], \n",
    "    ml_data['y_train'],\n",
    "    ml_data['X_test'], \n",
    "    ml_data['y_test']\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Model Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 5: FEATURE IMPORTANCE ANALYSIS\n",
    "# ================================================================================\n",
    "\n",
    "def extract_feature_importance(model_results, feature_names, feature_info, top_k=20):\n",
    "    \"\"\"\n",
    "    Extract and analyze feature importance from different models\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ðŸ”„ Extracting top {top_k} features from each model...\")\n",
    "    \n",
    "    feature_importance_dict = {}\n",
    "    \n",
    "    for model_name, result in model_results.items():\n",
    "        model = result['model']\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # Tree-based models\n",
    "            importances = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            # Linear models - use absolute values of coefficients\n",
    "            if len(model.coef_.shape) > 1:\n",
    "                # Multi-class: take mean of absolute coefficients\n",
    "                importances = np.mean(np.abs(model.coef_), axis=0)\n",
    "            else:\n",
    "                importances = np.abs(model.coef_)\n",
    "        else:\n",
    "            # For models without feature importance, use permutation importance\n",
    "            print(f\"  âš ï¸ {model_name} doesn't have built-in feature importance\")\n",
    "            continue\n",
    "        \n",
    "        # Create feature importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances,\n",
    "            'feature_type': [feature_type_map.get(f, 'Unknown') for f in feature_names]\n",
    "        })\n",
    "        \n",
    "        # Sort by importance and get top features\n",
    "        importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "        top_features = importance_df.head(top_k)\n",
    "        \n",
    "        feature_importance_dict[model_name] = {\n",
    "            'all_features': importance_df,\n",
    "            'top_features': top_features,\n",
    "            'top_rna': importance_df[importance_df['feature_type'] == 'RNA'].head(top_k),\n",
    "            'top_atac': importance_df[importance_df['feature_type'] == 'ATAC'].head(top_k)\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… {model_name}: Top feature = {top_features.iloc[0]['feature']} ({top_features.iloc[0]['importance']:.4f})\")\n",
    "    \n",
    "    return feature_importance_dict\n",
    "\n",
    "def analyze_cell_type_specific_features(X_train, y_train_labels, feature_info, top_k=10):\n",
    "    \"\"\"\n",
    "    Analyze top features for each cell type using statistical tests\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ðŸ”„ Analyzing cell-type specific features...\")\n",
    "    \n",
    "    cell_types = y_train_labels.unique()\n",
    "    cell_type_features = {}\n",
    "    \n",
    "    for cell_type in cell_types:\n",
    "        print(f\"  ðŸ“Š Analyzing {cell_type}...\")\n",
    "        \n",
    "        # Create binary labels for this cell type vs others\n",
    "        binary_labels = (y_train_labels == cell_type).astype(int)\n",
    "        \n",
    "        # Use SelectKBest with f_classif to find discriminative features\n",
    "        selector = SelectKBest(score_func=f_classif, k=min(top_k*5, X_train.shape[1]))\n",
    "        selector.fit(X_train, binary_labels)\n",
    "        \n",
    "        # Get feature scores\n",
    "        feature_scores = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'score': selector.scores_,\n",
    "            'p_value': selector.pvalues_,\n",
    "            'feature_type': [feature_type_map.get(f, 'Unknown') for f in X_train.columns]\n",
    "        })\n",
    "        \n",
    "        # Sort by score and filter significant features\n",
    "        significant_features = feature_scores[\n",
    "            feature_scores['p_value'] < 0.05\n",
    "        ].sort_values('score', ascending=False)\n",
    "        \n",
    "        cell_type_features[cell_type] = {
            'all_features': significant_features,
            'top_features': significant_features.head(top_k),
            'top_rna': significant_features[significant_features['feature_type'] == 'RNA'].head(top_k),
            'top_atac': significant_features[significant_features['feature_type'] == 'ATAC'].head(top_k)
        }
        
        print(f"    âœ… Found {len(significant_features)} significant features for {cell_type}")
    
    return cell_type_features

# Extract feature importance from models
feature_importance_results = extract_feature_importance(
    model_results, 
    ml_data['X_train'].columns, 
    feature_info, 
    top_k=20
)

# Analyze cell-type specific features
cell_type_feature_analysis = analyze_cell_type_specific_features(
    ml_data['X_train'], 
    ml_data['y_train_labels'], 
    feature_info, 
    top_k=15
)

print("\\nðŸŽ¯ Feature Analysis Complete!")\n",
    "            
